{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Taxi](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'output.png'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m#show image named output\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mIPython\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdisplay\u001b[39;00m \u001b[39mimport\u001b[39;00m Image\n\u001b[1;32m----> 3\u001b[0m Image(filename\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39moutput.png\u001b[39;49m\u001b[39m'\u001b[39;49m)\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\IPython\\core\\display.py:970\u001b[0m, in \u001b[0;36mImage.__init__\u001b[1;34m(self, data, url, filename, format, embed, width, height, retina, unconfined, metadata, alt)\u001b[0m\n\u001b[0;32m    968\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munconfined \u001b[39m=\u001b[39m unconfined\n\u001b[0;32m    969\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39malt \u001b[39m=\u001b[39m alt\n\u001b[1;32m--> 970\u001b[0m \u001b[39msuper\u001b[39;49m(Image, \u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(data\u001b[39m=\u001b[39;49mdata, url\u001b[39m=\u001b[39;49murl, filename\u001b[39m=\u001b[39;49mfilename,\n\u001b[0;32m    971\u001b[0m         metadata\u001b[39m=\u001b[39;49mmetadata)\n\u001b[0;32m    973\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata\u001b[39m.\u001b[39mget(\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m, {}):\n\u001b[0;32m    974\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mwidth \u001b[39m=\u001b[39m metadata[\u001b[39m'\u001b[39m\u001b[39mwidth\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\IPython\\core\\display.py:327\u001b[0m, in \u001b[0;36mDisplayObject.__init__\u001b[1;34m(self, data, url, filename, metadata)\u001b[0m\n\u001b[0;32m    324\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    325\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetadata \u001b[39m=\u001b[39m {}\n\u001b[1;32m--> 327\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreload()\n\u001b[0;32m    328\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_data()\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\IPython\\core\\display.py:1005\u001b[0m, in \u001b[0;36mImage.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Reload the raw data from file or URL.\"\"\"\u001b[39;00m\n\u001b[0;32m   1004\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membed:\n\u001b[1;32m-> 1005\u001b[0m     \u001b[39msuper\u001b[39;49m(Image,\u001b[39mself\u001b[39;49m)\u001b[39m.\u001b[39;49mreload()\n\u001b[0;32m   1006\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mretina:\n\u001b[0;32m   1007\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_retina_shape()\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\IPython\\core\\display.py:353\u001b[0m, in \u001b[0;36mDisplayObject.reload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    351\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m     encoding \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39mif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_read_flags \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mutf-8\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m--> 353\u001b[0m     \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfilename, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_flags, encoding\u001b[39m=\u001b[39;49mencoding) \u001b[39mas\u001b[39;00m f:\n\u001b[0;32m    354\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdata \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mread()\n\u001b[0;32m    355\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39murl \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    356\u001b[0m     \u001b[39m# Deferred import\u001b[39;00m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'output.png'"
     ]
    }
   ],
   "source": [
    "#show image named output\n",
    "from IPython.display import Image\n",
    "Image(filename='output.png')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Taxi](#toc1_)    \n",
    "- [Problem Description](#toc2_)    \n",
    "- [Pips](#toc3_)    \n",
    "- [Libraries](#toc4_)    \n",
    "- [Environment initializations](#toc5_)    \n",
    "- [Actions](#toc6_)    \n",
    "- [Reward System](#toc7_)    \n",
    "  - [Showing the map in a random state](#toc7_1_)    \n",
    "- [Functions](#toc8_)    \n",
    "- [Creating an agent that doesn't learn at all](#toc9_)    \n",
    "- [Training the Agent](#toc10_)    \n",
    "  - [Q-Table Method](#toc10_1_)    \n",
    "  - [Hyperparameters initialization](#toc10_2_)    \n",
    "- [Plotting training results](#toc11_)    \n",
    "- [Testing the model](#toc12_)    \n",
    "- [Final Result with Agent Trained](#toc13_)    \n",
    "- [Fine Tuning our Taxi](#toc14_)    \n",
    "- [DQN](#toc15_)    \n",
    "- [Sources](#toc16_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Taxi Problem from “Hierarchical Reinforcement Learning with the MAXQ Value Function Decomposition” by Tom Dietterich"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc2_'></a>[Problem Description](#toc0_)\n",
    "\n",
    "There are four designated locations in the grid world indicated by R(ed), G(reen), Y(ellow), and B(lue). When the episode starts, the taxi starts off at a random square and the passenger is at a random location. The taxi drives to the passenger’s location, picks up the passenger, drives to the passenger’s destination (another one of the four specified locations), and then drops off the passenger. Once the passenger is dropped off, the episode ends.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc3_'></a>[Pips](#toc0_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install numpy > /dev/null 2>&1\n",
    "!pip install gym > /dev/null 2>&1\n",
    "!pip install gymnasium\n",
    "!pip install cmake 'gym[atari]' scipy pygame\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install cmake gym[atari] scipy pygame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc4_'></a>[Libraries](#toc0_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "from time import sleep\n",
    "from matplotlib import animation\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc5_'></a>[Environment initializations](#toc0_)\n",
    "\n",
    "There are 500 discrete states since there are 25 taxi positions, 5 possible locations of the passenger (including the case when the passenger is in the taxi), and 4 destination locations.\n",
    "\n",
    "Note that there are 400 states that can actually be reached during an episode. The missing states correspond to situations in which the passenger is at the same location as their destination, as this typically signals the end of an episode. Four additional states can be observed right after a successful episodes, when both the passenger and the taxi are at the destination. This gives a total of 404 reachable discrete states.\n",
    "\n",
    "Each state space is represented by the tuple: (taxi_row, taxi_col, passenger_location, destination)\n",
    "\n",
    "An observation is an integer that encodes the corresponding state. The state tuple can then be decoded with the “decode” method.\n",
    "\n",
    "Passenger locations:\n",
    "\n",
    "- 0: R(ed)\n",
    "\n",
    "- 1: G(reen)\n",
    "\n",
    "- 2: Y(ellow)\n",
    "\n",
    "- 3: B(lue)\n",
    "\n",
    "- 4: in taxi"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install -U gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[box2d] in c:\\users\\juanhorrillo\\anaconda3\\envs\\reinforcelearning\\lib\\site-packages (0.26.2)\n",
      "Collecting version\n",
      "  Downloading version-0.1.1.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-plurh_zm\\version_f8fc9a6dffed4a9f92b62d75a7cbc040\\setup.py\", line 4, in <module>\n",
      "          from version import __version__\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-plurh_zm\\version_f8fc9a6dffed4a9f92b62d75a7cbc040\\version.py\", line 2, in <module>\n",
      "          from itertools import izip_longest\n",
      "      ImportError: cannot import name 'izip_longest' from 'itertools' (unknown location)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[atari] in c:\\users\\juanhorrillo\\anaconda3\\envs\\reinforcelearning\\lib\\site-packages (0.26.2)\n",
      "Collecting version\n",
      "  Using cached version-0.1.1.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-u_2hwlt9\\version_66cb73905aa8452fa3fd4fb8f6bc36c1\\setup.py\", line 4, in <module>\n",
      "          from version import __version__\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-u_2hwlt9\\version_66cb73905aa8452fa3fd4fb8f6bc36c1\\version.py\", line 2, in <module>\n",
      "          from itertools import izip_longest\n",
      "      ImportError: cannot import name 'izip_longest' from 'itertools' (unknown location)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[classic_control] in c:\\users\\juanhorrillo\\anaconda3\\envs\\reinforcelearning\\lib\\site-packages (0.26.2)\n",
      "Collecting version\n",
      "  Using cached version-0.1.1.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-6nea35z2\\version_6ffd0679bd3a45d0b1ee2bdc0d09fba8\\setup.py\", line 4, in <module>\n",
      "          from version import __version__\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-6nea35z2\\version_6ffd0679bd3a45d0b1ee2bdc0d09fba8\\version.py\", line 2, in <module>\n",
      "          from itertools import izip_longest\n",
      "      ImportError: cannot import name 'izip_longest' from 'itertools' (unknown location)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gym[all] in c:\\users\\juanhorrillo\\anaconda3\\envs\\reinforcelearning\\lib\\site-packages (0.26.2)\n",
      "Collecting version\n",
      "  Using cached version-0.1.1.tar.gz (2.0 kB)\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'error'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "  × python setup.py egg_info did not run successfully.\n",
      "  │ exit code: 1\n",
      "  ╰─> [8 lines of output]\n",
      "      Traceback (most recent call last):\n",
      "        File \"<string>\", line 2, in <module>\n",
      "        File \"<pip-setuptools-caller>\", line 34, in <module>\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-ckkx1bbd\\version_932eaedc602c4643914ed95c70225dac\\setup.py\", line 4, in <module>\n",
      "          from version import __version__\n",
      "        File \"C:\\Users\\JuanHorrillo\\AppData\\Local\\Temp\\pip-install-ckkx1bbd\\version_932eaedc602c4643914ed95c70225dac\\version.py\", line 2, in <module>\n",
      "          from itertools import izip_longest\n",
      "      ImportError: cannot import name 'izip_longest' from 'itertools' (unknown location)\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "error: metadata-generation-failed\n",
      "\n",
      "× Encountered error while generating package metadata.\n",
      "╰─> See above for output.\n",
      "\n",
      "note: This is an issue with the package mentioned above, not pip.\n",
      "hint: See above for details.\n"
     ]
    }
   ],
   "source": [
    "#pip install gym version 0.21\n",
    "!pip install gym[box2d] version 0.21\n",
    "!pip install gym[atari] version 0.21\n",
    "!pip install gym[classic_control] version 0.21\n",
    "!pip install gym[all] version 0.21\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'render_mode'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m env \u001b[39m=\u001b[39m gym\u001b[39m.\u001b[39;49mmake(\u001b[39m\"\u001b[39;49m\u001b[39mTaxi-v3\u001b[39;49m\u001b[39m\"\u001b[39;49m, render_mode\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mrgb_array\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39menv\n\u001b[0;32m      2\u001b[0m state, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mreset()\n\u001b[0;32m      4\u001b[0m \u001b[39m# Print dimensions of state and action space\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\gym\\envs\\registration.py:235\u001b[0m, in \u001b[0;36mmake\u001b[1;34m(id, **kwargs)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mmake\u001b[39m(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m--> 235\u001b[0m     \u001b[39mreturn\u001b[39;00m registry\u001b[39m.\u001b[39mmake(\u001b[39mid\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\gym\\envs\\registration.py:129\u001b[0m, in \u001b[0;36mEnvRegistry.make\u001b[1;34m(self, path, **kwargs)\u001b[0m\n\u001b[0;32m    127\u001b[0m     logger\u001b[39m.\u001b[39minfo(\u001b[39m\"\u001b[39m\u001b[39mMaking new env: \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m\"\u001b[39m, path)\n\u001b[0;32m    128\u001b[0m spec \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mspec(path)\n\u001b[1;32m--> 129\u001b[0m env \u001b[39m=\u001b[39m spec\u001b[39m.\u001b[39mmake(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    130\u001b[0m \u001b[39mreturn\u001b[39;00m env\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\gym\\envs\\registration.py:90\u001b[0m, in \u001b[0;36mEnvSpec.make\u001b[1;34m(self, **kwargs)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m     89\u001b[0m     \u001b[39mcls\u001b[39m \u001b[39m=\u001b[39m load(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mentry_point)\n\u001b[1;32m---> 90\u001b[0m     env \u001b[39m=\u001b[39m \u001b[39mcls\u001b[39m(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m_kwargs)\n\u001b[0;32m     92\u001b[0m \u001b[39m# Make the environment aware of which spec it came from.\u001b[39;00m\n\u001b[0;32m     93\u001b[0m spec \u001b[39m=\u001b[39m copy\u001b[39m.\u001b[39mdeepcopy(\u001b[39mself\u001b[39m)\n",
      "\u001b[1;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'render_mode'"
     ]
    }
   ],
   "source": [
    "\n",
    "env = gym.make(\"Taxi-v3\", render_mode=\"rgb_array\").env\n",
    "state, _ = env.reset()\n",
    "\n",
    "# Print dimensions of state and action space\n",
    "print(\"State space: {}\".format(env.observation_space))\n",
    "print(\"Action space: {}\".format(env.action_space))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc6_'></a>[Actions](#toc0_)\n",
    "\n",
    "There are 6 discrete deterministic actions:\n",
    "\n",
    "- 0: move south\n",
    "\n",
    "- 1: move north\n",
    "\n",
    "- 2: move east\n",
    "\n",
    "- 3: move west\n",
    "\n",
    "- 4: pickup passenger\n",
    "\n",
    "- 5: drop off passenger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TaxiEnv' object has no attribute 'action_mask'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[8], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Sample random action\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m action \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39maction_space\u001b[39m.\u001b[39msample(env\u001b[39m.\u001b[39;49maction_mask(state))\n\u001b[0;32m      3\u001b[0m next_state, reward, done, _, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Print output\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\JuanHorrillo\\anaconda3\\envs\\ReinforceLearning\\lib\\site-packages\\gym\\core.py:238\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m    234\u001b[0m \u001b[39mif\u001b[39;00m name\u001b[39m.\u001b[39mstartswith(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m    235\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mAttributeError\u001b[39;00m(\n\u001b[0;32m    236\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mattempted to get missing private attribute \u001b[39m\u001b[39m'\u001b[39m\u001b[39m{}\u001b[39;00m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(name)\n\u001b[0;32m    237\u001b[0m     )\n\u001b[1;32m--> 238\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39;49m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49menv, name)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TaxiEnv' object has no attribute 'action_mask'"
     ]
    }
   ],
   "source": [
    "# Sample random action\n",
    "action = env.action_space.sample(env.action_mask(state))\n",
    "next_state, reward, done, _, _ = env.step(action)\n",
    "\n",
    "# Print output\n",
    "print(\"Printing a random action:\")\n",
    "print(\"State: {}\".format(state))\n",
    "print(\"Action: {}\".format(action))\n",
    "print(\"Action mask: {}\".format(env.action_mask(state)))\n",
    "print(\"Reward: {}\".format(reward))\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc7_'></a>[Reward System](#toc0_)\n",
    "\n",
    "- -1 per step unless other reward is triggered.\n",
    "\n",
    "- +20 delivering passenger.\n",
    "\n",
    "- -10 executing “pickup” and “drop-off” actions illegally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Render the environment\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m env\u001b[39m.\u001b[39mrender()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Render the environment\n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc7_1_'></a>[Showing the map in a random state](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'env' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Render and plot an environment frame\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m frame \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mrender()\n\u001b[0;32m      3\u001b[0m plt\u001b[39m.\u001b[39mimshow(frame)\n\u001b[0;32m      4\u001b[0m plt\u001b[39m.\u001b[39maxis(\u001b[39m\"\u001b[39m\u001b[39moff\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'env' is not defined"
     ]
    }
   ],
   "source": [
    "# Render and plot an environment frame\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new instance of taxi, and get the initial state\n",
    "state = env.reset()\n",
    "frame = env.render()\n",
    "plt.imshow(frame)\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc8_'></a>[Functions](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_animation(experience_buffer):\n",
    "    \"\"\"Function to run animation\"\"\"\n",
    "    time_lag = 0.03  # Delay (in s) between frames\n",
    "    for experience in experience_buffer:\n",
    "        # Plot frame\n",
    "        clear_output(wait=True)\n",
    "        plt.imshow(experience['frame'])\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "\n",
    "        # Print console output\n",
    "        print(f\"Episode: {experience['episode']}/{experience_buffer[-1]['episode']}\")\n",
    "        print(f\"Epoch: {experience['epoch']}/{experience_buffer[-1]['epoch']}\")\n",
    "        print(f\"State: {experience['state']}\")\n",
    "        print(f\"Action: {experience['action']}\")\n",
    "        print(f\"Reward: {experience['reward']}\")\n",
    "        # Pauze animation\n",
    "        sleep(time_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def store_episode_as_gif(experience_buffer, path='./', filename='animation.gif'):\n",
    "    \"\"\"Store episode as gif animation\"\"\"\n",
    "    fps = 5   # Set framew per seconds\n",
    "    dpi = 300  # Set dots per inch\n",
    "    interval = 50  # Interval between frames (in ms)\n",
    "\n",
    "    # Retrieve frames from experience buffer\n",
    "    frames = []\n",
    "    for experience in experience_buffer:\n",
    "        frames.append(experience['frame'])\n",
    "\n",
    "    # Fix frame size\n",
    "    plt.figure(figsize=(frames[0].shape[1] / dpi, frames[0].shape[0] / dpi), dpi=dpi)\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "\n",
    "    # Generate animation\n",
    "    def animate(i):\n",
    "        patch.set_data(frames[i])\n",
    "\n",
    "    anim = animation.FuncAnimation(plt.gcf(), animate, frames=len(frames), interval=interval)\n",
    "\n",
    "    # Save output as gif\n",
    "    anim.save(path + filename, writer='imagemagick', fps=fps)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc9_'></a>[Creating an agent that doesn't learn at all](#toc0_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialization values\n",
    "epoch = 0\n",
    "num_failed_dropoffs = 0\n",
    "experience_buffer = []\n",
    "cum_reward = 0\n",
    "\n",
    "done = False\n",
    "\n",
    "state, _ = env.reset()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "while not done and epoch < 500:\n",
    "    # Sample random action\n",
    "    \"Action selection without action mask\"\n",
    "    action = env.action_space.sample()\n",
    "\n",
    "    \"Action selection with action mask\"\n",
    "    #action = env.action_space.sample(env.action_mask(state))\n",
    "\n",
    "    state, reward, done, _, _ = env.step(action)\n",
    "    cum_reward += reward\n",
    "\n",
    "    # Store experience in dictionary\n",
    "    experience_buffer.append({\n",
    "        \"frame\": env.render(),\n",
    "        \"episode\": 1,\n",
    "        \"epoch\": epoch,\n",
    "        \"state\": state,\n",
    "        \"action\": action,\n",
    "        \"reward\": cum_reward,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    if reward == -10:\n",
    "        num_failed_dropoffs += 1\n",
    "\n",
    "    epoch += 1"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this run, the agent will be stuck forever in episode 1, the agent will do a lot of iterations (epochs) through the 500 discrete states only able to gain a negative reward. Hence the taxi ends up moving randomly without picking the passenger."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_animation(experience_buffer)\n",
    "\n",
    "print(\"# epochs: {}\".format(epoch))\n",
    "print(\"# failed drop-offs: {}\".format(num_failed_dropoffs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The taxi is a disaster and ends up driving erratically forever"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc10_'></a>[Training the Agent](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc10_1_'></a>[Q-Table Method](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "#show q table\n",
    "print(q_table)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print dimensions o f q table\n",
    "print(\"Q table dimensions: {}\".format(q_table.shape))\n",
    " "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <a id='toc10_2_'></a>[Hyperparameters initialization](#toc0_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.1  # Learning rate\n",
    "gamma = 1.0  # Discount rate\n",
    "epsilon = 0.1  # # probability that our agent will explore\n",
    "num_episodes = 10000  # Number of episodes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Output for plots\n",
    "cum_rewards = np.zeros([num_episodes])\n",
    "total_epochs = np.zeros([num_episodes])\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    # Reset environment\n",
    "    state, info = env.reset()\n",
    "    epoch = 0 \n",
    "    num_failed_dropoffs = 0\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    "\n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            \"Basic exploration [~0.47m]\"\n",
    "            action = env.action_space.sample() # Sample random action (exploration)\n",
    "            \n",
    "            \"Exploration with action mask [~1.52m]\"\n",
    "          # action = env.action_space.sample(env.action_mask(state)) \"Exploration with action mask\"\n",
    "        else:      \n",
    "            \"Exploitation with action mask [~1m52s]\"\n",
    "           # action_mask = np.where(info[\"action_mask\"]==1,0,1) # invert\n",
    "           # masked_q_values = np.ma.array(q_table[state], mask=action_mask, dtype=np.float32)\n",
    "           # action = np.ma.argmax(masked_q_values, axis=0)\n",
    "\n",
    "            \"Exploitation with random tie breaker [~1m19s]\"\n",
    "          #  action = np.random.choice(np.flatnonzero(q_table[state] == q_table[state].max()))\n",
    "            \n",
    "            \"Basic exploitation [~47s]\"\n",
    "            action = np.argmax(q_table[state]) # Select best known action (exploitation)\n",
    " \n",
    "        next_state, reward, done, _ , info = env.step(action) \n",
    "\n",
    "        cum_reward += reward\n",
    "        \n",
    "        old_q_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, action] = new_q_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            num_failed_dropoffs += 1\n",
    "\n",
    "        state = next_state\n",
    "        epoch += 1\n",
    "        \n",
    "        total_epochs[episode-1] = epoch\n",
    "        cum_rewards[episode-1] = cum_reward\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode #: {episode}\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"===Training completed.===\\n\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc11_'></a>[Plotting training results](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot reward convergence\n",
    "plt.title(\"Cumulative reward per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"Cumulative reward\")\n",
    "plt.plot(cum_rewards)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot epoch convergence\n",
    "plt.title(\"# epochs per episode\")\n",
    "plt.xlabel(\"Episode\")\n",
    "plt.ylabel(\"# epochs\")\n",
    "plt.plot(total_epochs)\n",
    "plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc12_'></a>[Testing the model](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 0\n",
    "total_failed_deliveries = 0\n",
    "num_episodes = 1\n",
    "experience_buffer = []\n",
    "store_gif = True\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    # Initialize experience buffer\n",
    "\n",
    "    my_env = env.reset()\n",
    "    state = my_env[0]\n",
    "    epoch = 1 \n",
    "    num_failed_deliveries =0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        cum_reward += reward\n",
    "\n",
    "        if reward == -10:\n",
    "            num_failed_deliveries += 1\n",
    "\n",
    "        # Store rendered frame in animation dictionary\n",
    "        experience_buffer.append({\n",
    "            'frame': env.render(),\n",
    "            'episode': episode,\n",
    "            'epoch': epoch,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': cum_reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    total_failed_deliveries += num_failed_deliveries\n",
    "    num_epochs += epoch\n",
    "\n",
    "    if store_gif: #storing animation to trigger it in a separate cell\n",
    "        store_episode_as_gif(experience_buffer)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc13_'></a>[Final Result with Agent Trained](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Run animation and print output\n",
    "run_animation(experience_buffer)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\") \n",
    "print(f\"Test results after {num_episodes} episodes:\")\n",
    "print(f\"Mean # epochs per episode: {num_epochs / num_episodes}\")\n",
    "print(f\"Mean # failed drop-offs per episode: {total_failed_deliveries / num_episodes}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#env reset and render animation \n",
    "env.reset()\n",
    "env.render()\n",
    "#run animation\n",
    "run_animation(experience_buffer)\n",
    "#close environment\n",
    "env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fine Tuning our Taxi with Masked Q values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#finetune hyperparameters in train stage\n",
    "alpha = 0.1  # Learning rate\n",
    "gamma = 1.0  # Discount rate\n",
    "epsilon = 0.1  # # probability that our agent will explore\n",
    "num_episodes = 100000  # Number of episodes\n",
    "\n",
    " \n",
    "# Output for plots\n",
    "cum_rewards = np.zeros([num_episodes])\n",
    "total_epochs = np.zeros([num_episodes])\n",
    " \n",
    "for episode in range(1, num_episodes+1):\n",
    "    # Reset environment\n",
    "    state, info = env.reset()\n",
    "    epoch = 0 \n",
    "    num_failed_dropoffs = 0\n",
    "    done = False\n",
    "    cum_reward = 0\n",
    " \n",
    "    while not done:\n",
    "        \n",
    "        if random.uniform(0, 1) < epsilon:\n",
    "            \"Basic exploration [~0.47m]\"\n",
    "           # action = env.action_space.sample() # Sample random action (exploration)\n",
    "            \n",
    "            \"Exploration with action mask [~1.52m]\"\n",
    "          # action = env.action_space.sample(env.action_mask(state)) \"Exploration with action mask\"\n",
    "        else:      \n",
    "            \"Exploitation with action mask [~1m52s]\"\n",
    "            action_mask = np.where(info[\"action_mask\"]==1,0,1) # invert\n",
    "            masked_q_values = np.ma.array(q_table[state], mask=action_mask, dtype=np.float32)\n",
    "            action = np.ma.argmax(masked_q_values, axis=0)\n",
    "\n",
    "            \"Exploitation with random tie breaker [~1m19s]\"\n",
    "          #  action = np.random.choice(np.flatnonzero(q_table[state] == q_table[state].max()))\n",
    "            \n",
    "            \"Basic exploitation [~47s]\"\n",
    "            action = np.argmax(q_table[state]) # Select best known action (exploitation)\n",
    " \n",
    "        next_state, reward, done, _ , info = env.step(action) \n",
    "\n",
    "        cum_reward += reward\n",
    "        \n",
    "        old_q_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_q_value = (1 - alpha) * old_q_value + alpha * (reward + gamma * next_max)\n",
    "        \n",
    "        q_table[state, action] = new_q_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            num_failed_dropoffs += 1\n",
    "\n",
    "        state = next_state\n",
    "        epoch += 1\n",
    "        \n",
    "        total_epochs[episode-1] = epoch\n",
    "        cum_rewards[episode-1] = cum_reward\n",
    "\n",
    "    if episode % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode #: {episode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 0\n",
    "total_failed_deliveries = 0\n",
    "num_episodes = 1\n",
    "experience_buffer = []\n",
    "store_gif = True\n",
    "\n",
    "for episode in range(1, num_episodes+1):\n",
    "    # Initialize experience buffer\n",
    "\n",
    "    my_env = env.reset()\n",
    "    state = my_env[0]\n",
    "    epoch = 1 \n",
    "    num_failed_deliveries =0\n",
    "    cum_reward = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        action = np.argmax(q_table[state])\n",
    "        state, reward, done, _, _ = env.step(action)\n",
    "        cum_reward += reward\n",
    "\n",
    "        if reward == -10:\n",
    "            num_failed_deliveries += 1\n",
    "\n",
    "        # Store rendered frame in animation dictionary\n",
    "        experience_buffer.append({\n",
    "            'frame': env.render(),\n",
    "            'episode': episode,\n",
    "            'epoch': epoch,\n",
    "            'state': state,\n",
    "            'action': action,\n",
    "            'reward': cum_reward\n",
    "            }\n",
    "        )\n",
    "\n",
    "        epoch += 1\n",
    "\n",
    "    total_failed_deliveries += num_failed_deliveries\n",
    "    num_epochs += epoch\n",
    "\n",
    "    if store_gif: #storing animation to trigger it in a separate cell\n",
    "        store_episode_as_gif(experience_buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run animation and print output\n",
    "run_animation(experience_buffer)\n",
    "\n",
    "# Print final results\n",
    "print(\"\\n\") \n",
    "print(f\"Test results after {num_episodes} episodes:\")\n",
    "print(f\"Mean # epochs per episode: {num_epochs / num_episodes}\")\n",
    "print(f\"Mean # failed drop-offs per episode: {total_failed_deliveries / num_episodes}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparison with previous run:\n",
    "\n",
    "    Episode: 1/1\n",
    "    Epoch: 13/13\n",
    "    State: 85\n",
    "    Action: 5\n",
    "    Reward: 8\n",
    "\n",
    "\n",
    "    Test results after 1 episodes:\n",
    "    Mean # epochs per episode: 14.0\n",
    "    Mean # failed drop-offs per episode: 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc14_'></a>[Fine Tuning our Taxi with NN](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imporve training by using a neural network\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import load_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#build neural network\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_dim=env.observation_space.n, activation='relu'))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(Dense(env.action_space.n, activation='linear'))\n",
    "model.compile(loss='mse', optimizer=Adam())\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "   \n",
    "#train neural network\n",
    "def train_model(model, num_episodes, gamma, epsilon, epsilon_min, epsilon_decay):\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        state = env.reset()\n",
    "        state = np.reshape(state, [1, env.observation_space.n])\n",
    "        done = False\n",
    "        epoch = 1\n",
    "        cum_reward = 0\n",
    "\n",
    "        while not done:\n",
    "            if np.random.rand() <= epsilon:\n",
    "                action = random.randrange(env.action_space.n)\n",
    "            else:\n",
    "                q_values = model.predict(state)\n",
    "                action = np.argmax(q_values[0])\n",
    "\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.n])\n",
    "            cum_reward += reward\n",
    "\n",
    "            target = (reward + gamma * np.amax(model.predict(next_state)[0]))\n",
    "            target_f = model.predict(state)\n",
    "            target_f[0][action] = target\n",
    "\n",
    "            model.fit(state, target_f, epochs=1, verbose=0)\n",
    "\n",
    "            state = next_state\n",
    "            epoch += 1\n",
    "\n",
    "        if epsilon > epsilon_min:\n",
    "            epsilon *= epsilon_decay\n",
    "\n",
    "        if episode % 100 == 0:\n",
    "            clear_output(wait=True)\n",
    "            print(f\"Episode #: {episode}\")\n",
    "\n",
    "    print(\"\\n\")\n",
    "    print(\"===Training completed.===\\n\")\n",
    "\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train model\n",
    "trained_model = train_model(model, 10000, 0.9, 1.0, 0.01, 0.995)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test model\n",
    "def test_model(model, num_episodes):\n",
    "    num_epochs = 0\n",
    "    total_failed_dropoffs = 0\n",
    "    experience_buffer = []\n",
    "    store_gif = True\n",
    "\n",
    "    for episode in range(1, num_episodes+1):\n",
    "        my_env = env.reset()\n",
    "        state = my_env[0]\n",
    "        state = np.reshape(state, [1, env.observation_space.n])\n",
    "        epoch = 1 \n",
    "        num_failed_dropoffs =0\n",
    "        cum_reward = 0\n",
    "        done = False\n",
    "\n",
    "        while not done:\n",
    "            q_values = model.predict(state)\n",
    "            action = np.argmax(q_values[0])\n",
    "            next_state, reward, done, _, _ = env.step(action)\n",
    "            next_state = np.reshape(next_state, [1, env.observation_space.n])\n",
    "            cum_reward += reward\n",
    "\n",
    "            if reward == -10:\n",
    "                num_failed_dropoffs += 1\n",
    "\n",
    "            # Store rendered frame in animation dictionary\n",
    "            experience_buffer.append({\n",
    "                'frame': env.render(),\n",
    "                'episode': episode,\n",
    "                'epoch': epoch,\n",
    "                'state': state,\n",
    "                'action': action,\n",
    "                'reward': cum_reward\n",
    "                }\n",
    "            )\n",
    "\n",
    "            epoch += 1\n",
    "            state = next_state\n",
    "\n",
    "        total_failed_dropoffs += num_failed_dropoffs\n",
    "        num_epochs += epoch\n",
    "\n",
    "        if store_gif: #storing animation to trigger it in a separate cell\n",
    "            store_episode_as_gif(experience_buffer)\n",
    "\n",
    "    # Run animation and print output\n",
    "    run_animation(experience_buffer)\n",
    "\n",
    "    # Print final results\n",
    "    print(\"\\n\") \n",
    "    print(f\"Test results after {num_episodes} episodes:\")\n",
    "    print(f\"Mean # epochs per episode: {num_epochs / num_episodes}\")\n",
    "    print(f\"Mean # failed drop-offs per episode: {total_failed_dropoffs / num_episodes}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc15_'></a>[DQN](#toc0_)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install setuptools==65.5.0"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "!pip install stable_baselines3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTaxiEnv(gym.Wrapper):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "\n",
    "    def reset(self):\n",
    "        observation = super().reset()\n",
    "        return observation[0]  # Return only state\n",
    "\n",
    "    def step(self, action):\n",
    "        observation, reward, done, info = super().step(action)\n",
    "        return observation[0], reward, done, info  # Return only state\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_env = SimpleTaxiEnv(env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = DQN(\"MlpPolicy\", simple_env, learning_rate=0.001, verbose=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent.learn(total_timesteps=100000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "print(type(obs))\n",
    "print(obs)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import gym\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "env = gym.make('Taxi-v3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym.wrappers import TransformObservation\n",
    "\n",
    "def from_tuple(obs):\n",
    "    # If obs is a tuple, extract only the state integer\n",
    "    if isinstance(obs, tuple):\n",
    "        return obs[0]\n",
    "    # Otherwise, return obs as is\n",
    "    else:\n",
    "        return obs\n",
    "\n",
    "env = gym.make('Taxi-v3')\n",
    "env = TransformObservation(env, from_tuple)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "agent = DQN(\"MlpPolicy\", env, learning_rate=0.001, verbose=1)\n",
    "agent.learn(total_timesteps=100000)\n",
    "\n",
    "# Evaluate the agent's performance\n",
    "episode_rewards = []\n",
    "for _ in range(10):\n",
    "    episode_reward = 0\n",
    "    state = simple_env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        action, _ = agent.predict(state)  # Select only the action\n",
    "        new_state, reward, done, _ = simple_env.step(action)\n",
    "        episode_reward += reward\n",
    "        state = new_state\n",
    "    episode_rewards.append(episode_reward)\n",
    "\n",
    "print(\"Average reward:\", np.mean(episode_rewards))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN(\"MlpPolicy\", env, verbose=1)\n",
    "model.learn(total_timesteps=10000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_reward, _ = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "print(f\"Mean reward: {mean_reward:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "class Agent:\n",
    "\n",
    "    def __init__(self):\n",
    "        self.qnetwork = torch.nn.Sequential(\n",
    "            torch.nn.Linear(env.observation_space.shape[0], 128),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(128, env.action_space.n)\n",
    "        )\n",
    "        self.optimizer = torch.optim.Adam(self.qnetwork.parameters(), lr=0.0001)\n",
    "        self.memory = ReplayMemory(10000)\n",
    "        self.batch_size = 64\n",
    "        self.gamma = 0.99\n",
    "\n",
    "    def act(self, state):\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0)\n",
    "        action = self.qnetwork(state).argmax()\n",
    "        return action\n",
    "\n",
    "    def learn(self):\n",
    "        for _ in range(self.batch_size):\n",
    "            state, action, reward, next_state, done = self.memory.sample()\n",
    "\n",
    "            # Get the Q-values for the current state and the next state.\n",
    "            q_values = self.qnetwork(state)\n",
    "            q_next_values = self.qnetwork(next_state)\n",
    "\n",
    "            # Calculate the Q-target.\n",
    "            q_target = reward + (1 - done) * self.gamma * q_next_values.max()\n",
    "\n",
    "            # Calculate the loss.\n",
    "            loss = (q_values[action] - q_target)**2\n",
    "\n",
    "            # Update the Q-network parameters.\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Create the environment.\n",
    "    env = gym.make(\"Taxi-v3\")\n",
    "\n",
    "    # Create the agent.\n",
    "    agent = Agent()\n",
    "\n",
    "    # Train the agent.\n",
    "    for episode in range(10000):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.memory.add(state, action, reward, next_state, done)\n",
    "            if len(agent.memory) >= agent.batch_size:\n",
    "                agent.learn()\n",
    "            state = next_state\n",
    "\n",
    "    # Test the agent.\n",
    "    for _ in range(10):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            env.render()\n",
    "            action = agent.act(state)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            state = next_state\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#do the same with DQN\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from collections import deque\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.display import clear_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Hyperparameters\n",
    "BUFFER_SIZE = 100000\n",
    "BATCH_SIZE = 64\n",
    "GAMMA = 0.99\n",
    "TAU = 0.001\n",
    "LR = 0.0001\n",
    "UPDATE_EVERY = 4\n",
    "EPS_START = 1.0\n",
    "EPS_END = 0.01\n",
    "EPS_DECAY = 0.995\n",
    "NUM_EPISODES = 10000\n",
    "MAX_T = 1000\n",
    "PRINT_EVERY = 100\n",
    "SEED = 0\n",
    "STORE_GIF = True\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = (state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "\n",
    "        # Convert batch of tuples to tuple of batches\n",
    "        states, actions, rewards, next_states, dones = zip(*experiences)\n",
    "        states = torch.from_numpy(np.array(states)).float().to(device)\n",
    "        actions = torch.from_numpy(np.array(actions)).long().to(device)\n",
    "        rewards = torch.from_numpy(np.array(rewards)).float().to(device)\n",
    "        next_states = torch.from_numpy(np.array(next_states)).float().to(device)\n",
    "        dones = torch.from_numpy(np.array(dones).astype(np.uint8)).float().to(device)\n",
    "\n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "    \n",
    "# Q-network\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"Actor (Policy) Model.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, fc1_units=64, fc2_units=64, fc3_units=64):\n",
    "        \"\"\"Initialize parameters and build model.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int):  Number of states\n",
    "            action_size (int): Number of actions\n",
    "            seed (int):        Random seed\n",
    "            fc1_units (int):   Number of nodes in first hidden layer\n",
    "            fc2_units (int):   Number of nodes in second hidden layer\n",
    "            fc3_units (int):   Number of nodes in third hidden layer\n",
    "        \"\"\"\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.seed = torch.manual_seed(seed)\n",
    "        self.fc1 = nn.Linear(state_size + action_size, fc1_units)\n",
    "        self.fc2 = nn.Linear(fc1_units, fc2_units)\n",
    "        #self.fc3 = nn.Linear(fc2_units, fc3_units)\n",
    "        self.fc3 = nn.Linear(fc2_units, 1)\n",
    "        \n",
    "    def forward(self, state, action):\n",
    "        \"\"\"Build a network that maps state-action pairs -> Q-values.\"\"\"\n",
    "        x = torch.cat((state, action), dim=1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        #x = F.relu(self.fc3(x))\n",
    "        return self.fc3(x)\n",
    "    \n",
    "# Agent\n",
    "class Agent():\n",
    "    \"\"\"Interacts with and learns from the environment.\"\"\"\n",
    "\n",
    "    def __init__(self, state_size, action_size, seed, batch_size=BATCH_SIZE, gamma=GAMMA, tau=TAU, lr=LR, update_every=UPDATE_EVERY):\n",
    "        \"\"\"Initialize an Agent object.\n",
    "        Params\n",
    "        ======\n",
    "            state_size (int):   Number of states\n",
    "            action_size (int):  Number of actions\n",
    "            seed (int):         Random seed\n",
    "            batch_size (int):   Batch size\n",
    "            gamma (float):      Discount factor\n",
    "            tau (float):        For soft update of target parameters\n",
    "            lr (float):         Learning rate \n",
    "            update_every (int): How often to update the network\n",
    "        \"\"\"\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.batch_size = batch_size\n",
    "        self.gamma = gamma\n",
    "        self.tau = tau\n",
    "        self.lr = lr\n",
    "        self.update_every = update_every\n",
    "        \n",
    "        # Q-Network\n",
    "        self.qnetwork_local = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = QNetwork(state_size, action_size, seed).to(device)\n",
    "        self.optimizer = optim.Adam(self.qnetwork_local.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Replay memory\n",
    "        self.memory = ReplayBuffer(BUFFER_SIZE, self.batch_size, seed)\n",
    "        \n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "    \n",
    "    def act(self, state, eps=0.):\n",
    "        \"\"\"Returns actions for given state as per current policy.\n",
    "        Params\n",
    "        ======\n",
    "            state (array_like): Current state\n",
    "            eps (float):        Epsilon, for epsilon-greedy action selection\n",
    "        \"\"\"\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_local.eval()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_local(state)\n",
    "        \n",
    "        self.qnetwork_local.train()\n",
    "        \n",
    "        # Epsilon-greedy action selection\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "    \n",
    "    def step(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Save experience in replay memory, and use random sample from buffer to learn.\"\"\"\n",
    "        # Save experience\n",
    "        self.memory.add(state, action, reward, next_state, done)\n",
    "        \n",
    "        # Learn every UPDATE_EVERY time steps\n",
    "        self.t_step = (self.t_step + 1) % self.update_every\n",
    "        \n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get random subset and learn\n",
    "            if len(self.memory) > self.batch_size:\n",
    "                experiences = self.memory.sample()\n",
    "                self.learn(experiences, self.gamma) \n",
    "                \n",
    "                    \n",
    "                 \n",
    "    def learn(self, experiences, gamma):\n",
    "        \"\"\"Update value parameters using given batch of experience tuples.\n",
    "        Params\n",
    "        ======\n",
    "            experiences (Tuple[torch.Tensor]): Tuple of (s, a, r, s', done) tuples \n",
    "            gamma (float):                     Discount factor\n",
    "        \"\"\"\n",
    "        states, actions, rewards, next_states, dones = experiences\n",
    "        \n",
    "        # Get max predicted Q values (for next states) from target model\n",
    "        Q_targets_next = self.qnetwork_target(next_states, self.qnetwork_local(next_states).detach())\n",
    "        \n",
    "        # Compute Q targets for current states \n",
    "        Q_targets = rewards + (gamma * Q_targets_next * (1 - dones))\n",
    "        \n",
    "        # Get expected Q values from local model\n",
    "        Q_expected = self.qnetwork_local(states, actions)\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = F.mse_loss(Q_expected, Q_targets)\n",
    "        \n",
    "        # Minimize the loss\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Update target network\n",
    "        self.soft_update(self.qnetwork_local, self.qnetwork_target, self.tau)\n",
    "        \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "        \"\"\"Soft update model parameters.\n",
    "        θ_target = τ*θ_local + (1 - τ)*θ_target\n",
    "        Params\n",
    "        ======\n",
    "            local_model (PyTorch model):  Weights will be copied from\n",
    "            target_model (PyTorch model): Weights will be copied to\n",
    "            tau (float):                 Interpolation parameter \n",
    "        \"\"\"\n",
    "        for target_param, local_param in zip(target_model.parameters(), local_model.parameters()):\n",
    "            target_param.data.copy_(tau * local_param.data + (1.0 - tau) * target_param.data)\n",
    "              \n",
    "# Replay buffer\n",
    "class ReplayBuffer:\n",
    "    \"\"\"Fixed-size buffer to store experience tuples.\"\"\"\n",
    "\n",
    "    def __init__(self, buffer_size, batch_size, seed):\n",
    "        \"\"\"Initialize a ReplayBuffer object.\n",
    "        Params\n",
    "        ======\n",
    "            buffer_size (int): Size of buffer\n",
    "            batch_size (int):  Size of each training batch\n",
    "            seed (int):        Random seed\n",
    "        \"\"\"\n",
    "        self.memory = deque(maxlen=buffer_size)  \n",
    "        self.batch_size = batch_size\n",
    "        self.seed = random.seed(seed)\n",
    "        self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
    "    \n",
    "    def add(self, state, action, reward, next_state, done):\n",
    "        \"\"\"Add a new experience to memory.\"\"\"\n",
    "        e = self.experience(state, action, reward, next_state, done)\n",
    "        self.memory.append(e)\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"Randomly sample a batch of experiences from memory.\"\"\"\n",
    "        experiences = random.sample(self.memory, k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).float().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states, actions, rewards, next_states, dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        \"\"\"Return the current size of internal memory.\"\"\"\n",
    "        return len(self.memory)\n",
    "     \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agent\n",
    "agent = Agent(state_size=state_size, action_size=action_size, seed=0)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "\n",
    "    scores = []                        # List of scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # Last 100 scores\n",
    "    eps = eps_start                    # Initialize epsilon\n",
    "    \n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        # Reset environment\n",
    "        env_info = env.reset(train_mode=True)[brain_name]\n",
    "        \n",
    "        # Get current state\n",
    "        state = env_info.vector_observations[0]\n",
    "        \n",
    "        # Initialize score\n",
    "        score = 0\n",
    "        \n",
    "        for t in range(max_t):\n",
    "            # Select action\n",
    "            action = agent.act(state, eps)\n",
    "            \n",
    "            # Take action\n",
    "            env_info = env.step(action)[brain_name]\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = env_info.vector_observations[0]\n",
    "            \n",
    "            # Get reward\n",
    "            reward = env_info.rewards[0]\n",
    "            \n",
    "            # Check if episode has finished\n",
    "            done = env_info.local_done[0]\n",
    "            \n",
    "            # Update score\n",
    "            score += reward\n",
    "            \n",
    "            # Update state\n",
    "            state = next_state\n",
    "            \n",
    "            # Update agent\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            \n",
    "            # Check if episode has finished\n",
    "            if done:\n",
    "                break\n",
    "        \n",
    "        # Save most recent score\n",
    "        scores_window.append(score)\n",
    "        \n",
    "        # Save most recent score\n",
    "        scores.append(score)\n",
    "        \n",
    "        # Decrease epsilon\n",
    "        eps = max(eps_end, eps_decay*eps)\n",
    "        \n",
    "        # Print results\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        \n",
    "        # Print results every 100 episodes\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        \n",
    "        # Check if environment is solved\n",
    "        if np.mean(scores_window)>=13.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            \n",
    "            # Save weights\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            \n",
    "            break\n",
    "    \n",
    "    return scores\n",
    "\n",
    "# Train agent\n",
    "scores = dqn()\n",
    "    \n",
    "# Plot scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "   \n",
    "# Close environment\n",
    "env.close()\n",
    "\n",
    "# Save scores\n",
    "with open('scores.txt', 'w') as f:\n",
    "    for score in scores:\n",
    "        f.write(\"%s\\n\" % score)\n",
    "         \n",
    "          \n",
    "           \n",
    "           "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <a id='toc16_'></a>[Sources](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/solving-the-taxi-environment-with-q-learning-a-tutorial-c76c22fc5d8f"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
